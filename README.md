# webCrawler
获取当前租房网站的房源信息


2020/3/16

1、中间件中设置随机代理

2、分析目标网站
    
    链家北京信息第一页    https://bj.lianjia.com/zufang/pg1/#contentList
               第二页    https://bj.lianjia.com/zufang/pg2/#contentList
    因此需要拼接页数

2020/3/25
  
1、使用管道将数据存储到远程数据库之中

2、发现的问题：链家爬虫只显示三千条数据，其他的会隐藏，无法获取。所有可能要更改代码，
解决这个问题。例如对每个区域进行手动输入链接，对每个区域进行开始爬虫，但仍然获取不了全部信息
。网上有将小区作为链接开始进行爬取，但小区数量有几百个，挨个获取连接太麻烦。

3、待解决问题：增量爬虫，设置对比算法，对每日爬取数据进行对比，新增数据添加数据库，修改的数据进行更改。

4、已经解决了增量问题，即在管道中先查询数据库有无此数据，没有数据才会添加到数据库

2020/3/26

1、添加商圈链接到数据库中，目的是以后根据商圈链接找到商圈的小区并且获取信息。需要注意链家商圈295个左右，
但是有许多商圈在不同的行政区的名字相同，相同名字的商圈的链接共用，所以我们对相同名字的商圈只爬取一次。
 
 2020/3/27
 
 1、封装了数据库操作
 
 2、添加了日志记录并封装
 
 2020/3/28
 
 1、lianjia_community.py对所有小区信息进行获取，已经设置了增量爬取、断点续爬、存入数据库进行去重。但是
 由于scarpy是异步爬取，所以在设置商圈已经被爬取标志时间不确定，所以可能出现这种情况：在爬取中商圈部分信息爬取
 完成就已经设置商圈链接被访问，就在此刻爬虫中断，所以下次爬虫开始忽略了尚未爬取完成的商圈。
 
 2、可以发布到服务器上
 
 2020/3/31
 
 1、添加爬取租房信息，用了22个小时爬取了三千六百条数据，但查看日志程序 是正常结束。
 幸运的是本就设置了增量爬虫，又重新启动爬虫，果然仍有些数据没有爬取下来，所以再次启动爬虫
 预计可以爬取四万条数据